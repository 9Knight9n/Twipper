{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9Knight9n/Twipper/blob/main/colab/Copy_of_trend_prediction_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG9ZdGa53vHe"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "import random\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, SpatialDropout1D, Embedding\n",
        "from tensorflow.keras.layers import concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "# from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.model_selection import KFold\n",
        "import os\n",
        "from gensim.models import KeyedVectors\n",
        "!pip install keras-tcn\n",
        "from tcn import TCN\n",
        "import json\n",
        "from datetime import datetime\n",
        "import string\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq-WkSIjjaL9"
      },
      "outputs": [],
      "source": [
        "# Global variables\n",
        "%config IPCompleter.greedy=True\n",
        "%config IPCompleter.use_jedi=False\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \" \"\n",
        "\n",
        "LDA_SIZE = 8\n",
        "TRENDS_NUMBER = 5\n",
        "TEXT_TOKENIZER_NUM_WORDS=1000\n",
        "TREND_TOKENIZER_NUM_WORDS=1000\n",
        "TRAIN_SIZE = 0.7\n",
        "TEST_SIZE = 0.15\n",
        "CLUSTER_SIZE = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UEsziWVX1Zq8"
      },
      "outputs": [],
      "source": [
        "# download and unzip required files  \n",
        "if not os.path.exists('trend_prediction.csv') or not os.path.exists('trends.csv'):\n",
        "    !curl -s -O https://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@raw.githubusercontent.com/9Knight9n/Twipper/main/colab/data.zip\n",
        "    !unzip data.zip\n",
        "    !rm data.zip\n",
        "if not os.path.exists('GoogleNews-vectors-negative300.bin.gz'):\n",
        "    !gdown 0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "# if not os.path.exists('j_trends.csv'):\n",
        "#     !gdown 1pDFZIK-dwmIJW0S4GygccAq7PtF_jDd6\n",
        "# if not os.path.exists('timelines_en_v3.csv'):\n",
        "#     !gdown 1VbnY99lmPOImXbsV4H00wD3NL4CbxdJE\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def convert_hashtag(text:str):\n",
        "#     text = text.replace(\"_\",\" \")\n",
        "#     if not text.startswith('#'):\n",
        "#         return text.lower()\n",
        "#     result = ''\n",
        "#     for i in range(len(text)):\n",
        "#         ch = text[i]\n",
        "#         if ch.isupper() or ch.isdigit():\n",
        "#             if i-1 > 0 and text[i-1].isupper():\n",
        "#                 continue\n",
        "#             result += ' '\n",
        "#         is_digit = False\n",
        "#         while ch.isdigit() and i < len(text) - 1:\n",
        "#             is_digit = True\n",
        "#             i += 1\n",
        "#             ch = text[i]\n",
        "#             result += ch\n",
        "#         if is_digit:\n",
        "#             result += ' '\n",
        "#         else:\n",
        "#             result += ch\n",
        "#     return result.lower().replace(\"  \",\" \")\n",
        "\n",
        "\n",
        "# def tweet_preprocess(text:str):\n",
        "#     min_length = 10\n",
        "#     text = text.replace('&amp',' ')\n",
        "#     text = re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)', '', text)  # remove links\n",
        "#     text = re.sub('\\S*@\\S*\\s?', '', text)  # remove emails\n",
        "#     text = re.sub('\\s+', ' ', text)  # remove newline chars\n",
        "#     text = simple_preprocess(str(text), deacc=True)\n",
        "#     text = [convert_hashtag(word) for word in text]\n",
        "#     text = [re.sub('[^ a-z\\d]', '', word.strip()) for word in text]  # remove non english characters\n",
        "#     stop_words = stopwords.words('english')\n",
        "#     text = [word for word in text if word not in stop_words]  # remove stopwords\n",
        "#     text = [word for word in text if len(word) > 2]  # remove short words\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     text = [lemmatizer.lemmatize(word) for word in text if len(word) > 1]\n",
        "#     text = ' '.join(text)\n",
        "#     if len(text) < min_length:\n",
        "#         return ''\n",
        "#     return text"
      ],
      "metadata": {
        "id": "RdVOW6KBGMdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR_L84r5sgSt"
      },
      "outputs": [],
      "source": [
        "# prepare data\n",
        "df = pd.read_csv('trend_prediction.csv', delimiter=',')\n",
        "data = [tuple(x) for x in df.values]\n",
        "x = np.array([item[2] for item in data])\n",
        "y = np.array([json.loads(item[4]) for item in data],dtype=object)\n",
        "\n",
        "x_train = x[:int(TRAIN_SIZE*len(x))]\n",
        "x_test = x[int(TRAIN_SIZE*len(x)):int((TRAIN_SIZE+TEST_SIZE)*len(x))]\n",
        "x_validate = x[int((TRAIN_SIZE+TEST_SIZE)*len(x)):]\n",
        "\n",
        "y_train_temp = y[:int(TRAIN_SIZE*len(y))]\n",
        "y_test_temp = y[int(TRAIN_SIZE*len(y)):int((TRAIN_SIZE+TEST_SIZE)*len(y))]\n",
        "y_validate_temp = y[int((TRAIN_SIZE+TEST_SIZE)*len(y)):]\n",
        "\n",
        "classes = []\n",
        "for item in y:\n",
        "    classes += list(item)\n",
        "classes = np.array(classes)\n",
        "print(classes.shape)\n",
        "classes = np.unique(classes)\n",
        "print(classes.shape)\n",
        "\n",
        "\n",
        "df = pd.read_csv('trends.csv', delimiter=',')\n",
        "trends = [tuple(x) for x in df.values]\n",
        "trends = {item[1]:item[2] for item in trends if item[1] in classes}\n",
        "print(len(trends.keys()))\n",
        "\n",
        "\n",
        "# df = pd.read_csv('timelines_en_v3.csv', delimiter=',')\n",
        "# timelines_raw_data = [tuple(x) for x in df.values]\n",
        "# df = pd.read_csv('j_trends.csv', delimiter=',', lineterminator='\\n')\n",
        "# tweets_raw_data = [tuple(x) for x in df.values]\n",
        "# dates_text = {}\n",
        "# for index,data in enumerate(timelines_raw_data):\n",
        "#     if not isinstance(data[3],str):\n",
        "#         continue\n",
        "#     date = datetime.strptime(data[3].replace('+00:00',''), '%Y-%m-%d %H:%M:%S').date()\n",
        "#     date = str(date)\n",
        "#     if date in dates_text.keys():\n",
        "#         dates_text[date].append(index)\n",
        "#     else:\n",
        "#         dates_text[date] = [index]\n",
        "# # print(dates.keys())\n",
        "\n",
        "# new_dates = {}\n",
        "# for key,value in dates_text.items():\n",
        "#     text = ' '.join([tweet_preprocess(timelines_raw_data[index][2]) for index in value if isinstance(timelines_raw_data[index][2],str)])\n",
        "#     new_dates[key] = text\n",
        "\n",
        "# dates_text = new_dates\n",
        "\n",
        "        \n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(x_validate.shape)\n",
        "print(y_train_temp.shape)\n",
        "print(y_test_temp.shape)\n",
        "print(y_validate_temp.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for key,value in dates.items():\n",
        "#     print(value[:1000])\n",
        "#     break\n",
        "# for i in range(10):\n",
        "#     print(tweets_raw_data[i])\n",
        "\n",
        "# dates_trend = {}\n",
        "# for index,data in enumerate(tweets_raw_data):\n",
        "#     if not isinstance(data[1],str):\n",
        "#         continue\n",
        "#     date = datetime.strptime(\" \".join(data[1].replace('+0000 ','').split(' ')[1:]), '%b %d %H:%M:%S %Y').date()\n",
        "#     if date in dates_trend.keys():\n",
        "#         dates_trend[date].append(data[10])\n",
        "#     else:\n",
        "#         dates_trend[date] = [data[10]]\n",
        "# # print(dates.keys())\n",
        "\n",
        "# new_dates = {}\n",
        "# for key,value in dates_trend.items():\n",
        "#     new_dates[key] = list(set(value))\n",
        "\n",
        "# dates_trend = new_dates\n",
        "# print(dates_trend['2011-03-02'])"
      ],
      "metadata": {
        "id": "SFrfOjC3NRIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvczDMtDuF0Q"
      },
      "outputs": [],
      "source": [
        "# Define a required function \n",
        "def max_length(sequences):\n",
        "    max_length = 0\n",
        "    for i, seq in enumerate(sequences):\n",
        "        length = len(seq)\n",
        "        if max_length < length:\n",
        "            max_length = length\n",
        "    return max_length\n",
        "\n",
        "\n",
        "def y_to_vector(trends,classes_to_cluster_dict,cluster_size):\n",
        "    result = np.zeros(shape=(trends.shape[0],cluster_size))\n",
        "    print(result.shape)\n",
        "    for i in range(trends.shape[0]):\n",
        "        trend = trends[i]\n",
        "        # print(trend)\n",
        "        for item in trend:\n",
        "            result[i][classes_to_cluster_dict[item]] = 1.0\n",
        "    return result\n",
        "\n",
        "\n",
        "def pretrained_embedding_matrix(word_to_vec_map, tokenizer, emb_mean, emb_std):\n",
        "    '''\n",
        "    input:\n",
        "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
        "        word_to_index: word to index mapping from training set\n",
        "    '''\n",
        "    np.random.seed(2022)\n",
        "    \n",
        "    # define dimensionality of your pre-trained word vectors (= 300)\n",
        "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
        "    \n",
        "    # initialize the matrix with generic normal distribution values\n",
        "    embed_matrix = np.random.normal(emb_mean, emb_std, (tokenizer.num_words, emb_dim))\n",
        "    \n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for idx, (word, count) in enumerate(tokenizer.word_counts.items()):\n",
        "        if idx < tokenizer.num_words-1 and word in word_to_vec_map:\n",
        "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
        "            \n",
        "    return embed_matrix\n",
        "\n",
        "\n",
        "\n",
        "def tcn_model(kernel_size = 3, activation='sigmoid', input_dim = None, \n",
        "                   output_dim=300, max_length = None, emb_matrix = None):\n",
        "    \n",
        "    inp = Input( shape=(max_length,))\n",
        "    x = Embedding(input_dim=input_dim, \n",
        "                  output_dim=output_dim, \n",
        "                  input_length=max_length,\n",
        "                  # Assign the embedding weight with word2vec embedding marix\n",
        "                  weights = [emb_matrix],\n",
        "                  # Set the weight to be not trainable (static)\n",
        "                  trainable = False)(inp)\n",
        "    \n",
        "    x = SpatialDropout1D(0.1)(x)\n",
        "    \n",
        "    x = TCN(256,dilations = [1, 2, 4], return_sequences=True, activation = activation)(x)\n",
        "    x = TCN(128,dilations = [1, 2, 4], return_sequences=True, activation = activation)(x)\n",
        "    \n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    \n",
        "    conc = concatenate([avg_pool, max_pool])\n",
        "    conc = Dense(128, activation=\"sigmoid\")(conc)\n",
        "    conc = Dropout(0.2)(conc)\n",
        "    outp = Dense(CLUSTER_SIZE, activation=\"sigmoid\")(conc)    \n",
        "\n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss = 'binary_crossentropy', \n",
        "                  optimizer = 'adam', metrics = ['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_ojUmY1frg_"
      },
      "outputs": [],
      "source": [
        "\n",
        "EMBEDDING_FILE = './GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "emb_mean = word2vec.vectors.mean()\n",
        "emb_std = word2vec.vectors.std()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trend_embedder(input_dim = None,\n",
        "                   output_dim=300, max_length = None, emb_matrix = None):\n",
        "    \n",
        "    inp = Input( shape=(max_length,))\n",
        "    x = Embedding(input_dim=input_dim, \n",
        "                  output_dim=output_dim, \n",
        "                  input_length=max_length,\n",
        "                  # Assign the embedding weight with word2vec embedding marix\n",
        "                  weights = [emb_matrix],\n",
        "                  # Set the weight to be not trainable (static)\n",
        "                  trainable = False)(inp)\n",
        "\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "\n",
        "    \n",
        "    return model\n",
        "\n",
        "# TREND_TOKENIZER_NUM_WORDS=1000\n",
        "# Define the trend embedder\n",
        "# Cleaning and Tokenization\n",
        "trend_values = trends.values()\n",
        "trend_keys = list(trends.keys())\n",
        "trend_tokenizer = Tokenizer(oov_token=oov_tok,lower=True,num_words=TREND_TOKENIZER_NUM_WORDS)\n",
        "trend_tokenizer.fit_on_texts(trend_values)\n",
        "\n",
        "# Turn the text into sequence\n",
        "trend_sequences = trend_tokenizer.texts_to_sequences(trend_values)\n",
        "\n",
        "# import collections\n",
        "# \n",
        "# max_length_ = 100 \n",
        "# print(trend_sequences)\n",
        "new_trend_sequences = []\n",
        "for index,trend_sequence in enumerate(trend_sequences):\n",
        "    while 1 in trend_sequence:trend_sequence.remove(1)\n",
        "#     size_ = len(trend_sequence)\n",
        "#     # print(size_)\n",
        "#     if size_ > max_length_:\n",
        "#         div = max_length_/size_\n",
        "#         repetition = {item:count for item, count in collections.Counter(trend_sequence).items()}\n",
        "#         for word,count in repetition.items():\n",
        "#             for i in range(int((1-div)*count*2)):\n",
        "#                 if word in trend_sequence:trend_sequence.remove(word)\n",
        "    new_trend_sequences.append(trend_sequence)\n",
        "trend_sequences = new_trend_sequences\n",
        "# print(trend_sequences)\n",
        "\n",
        "# trend_max_len = max_length(trend_sequences)\n",
        "trend_max_len = 1000\n",
        "print(trend_max_len)\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "trend_embedder_x = pad_sequences(trend_sequences, maxlen=trend_max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "# word_index = tokenizer.word_index\n",
        "# vocab_size = len(word_index)+1\n",
        "# print(vocab_size)\n",
        "\n",
        "\n",
        "trend_emb_matrix = pretrained_embedding_matrix(word2vec, trend_tokenizer, emb_mean, emb_std)\n",
        "trend_embedder = trend_embedder(input_dim=TREND_TOKENIZER_NUM_WORDS, output_dim = 300,\n",
        "                                max_length=trend_max_len,\n",
        "                                emb_matrix=trend_emb_matrix)\n",
        "\n",
        "trends_embedding = trend_embedder.predict(trend_embedder_x)\n",
        "print(trends_embedding.shape)\n",
        "trends_embedding_2d = trends_embedding.reshape((trends_embedding.shape[0],trends_embedding.shape[1]*trends_embedding.shape[2]))\n",
        "print(trends_embedding_2d.shape)"
      ],
      "metadata": {
        "id": "GJdjbBcDau7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "k_means = KMeans(n_clusters=CLUSTER_SIZE, random_state=0).fit(trends_embedding_2d)\n",
        "k_means = list(k_means.labels_)\n",
        "print(len(trend_values))\n",
        "print(len(k_means))\n",
        "classes_to_cluster_dict = {trend_keys[i]:k_means[i] for i in range(len(trend_keys))}\n",
        "\n",
        "for i in range(CLUSTER_SIZE):\n",
        "    print(f'cluster {i} :',end=\" \")\n",
        "    for key,item in classes_to_cluster_dict.items():\n",
        "        if item == i:\n",
        "            print(f'\"{key}\" ',end=', ')\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "y_train = y_to_vector(y_train_temp,classes_to_cluster_dict,CLUSTER_SIZE)\n",
        "y_test = y_to_vector(y_test_temp,classes_to_cluster_dict,CLUSTER_SIZE)\n",
        "y_validate = y_to_vector(y_validate_temp,classes_to_cluster_dict,CLUSTER_SIZE)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_validate.shape)"
      ],
      "metadata": {
        "id": "qQWFi3E3kNiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sse={}\n",
        "# for k in range(1, 50):\n",
        "#     kmeans = KMeans(n_clusters=k, max_iter=1000).fit(trends_embedding_2d)\n",
        "#     #print(data[\"clusters\"])\n",
        "#     sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\n",
        "# plt.figure()\n",
        "# plt.plot(list(sse.keys()), list(sse.values()))\n",
        "# plt.xlabel(\"Number of cluster\")\n",
        "# plt.ylabel(\"SSE\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "50yMSvVtivil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ_K-IQIgD7P"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# encode data using\n",
        "# Cleaning and Tokenization\n",
        "tokenizer = Tokenizer(oov_token=oov_tok,lower=True,num_words=TEXT_TOKENIZER_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "# Turn the text into sequence\n",
        "training_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
        "validate_sequences = tokenizer.texts_to_sequences(x_validate)\n",
        "\n",
        "\n",
        "# import collections\n",
        "\n",
        "# max_length_ = 10000 \n",
        "# new_sequences = []\n",
        "# # print(len(training_sequences))\n",
        "# c = 1\n",
        "# for trend_sequence in training_sequences:\n",
        "#     print(f'{c} out of {len(training_sequences)}')\n",
        "#     c += 1\n",
        "#     while 1 in trend_sequence:trend_sequence.remove(1)\n",
        "#     size_ = len(trend_sequence)\n",
        "#     if size_ > max_length_:\n",
        "#         div = max_length_/size_\n",
        "#         repetition = {item:count for item, count in collections.Counter(trend_sequence).items()}\n",
        "#         for word,count in repetition.items():\n",
        "#             for i in range(int((1-div)*count*2)):\n",
        "#                 if word in trend_sequence:trend_sequence.remove(word)\n",
        "#     new_sequences.append(trend_sequence)\n",
        "# training_sequences = new_sequences\n",
        "# new_sequences = []\n",
        "# for trend_sequence in test_sequences:\n",
        "#     while 1 in trend_sequence:trend_sequence.remove(1)\n",
        "#     size_ = len(trend_sequence)\n",
        "#     if size_ > max_length_:\n",
        "#         div = max_length_/size_\n",
        "#         repetition = {item:count for item, count in collections.Counter(trend_sequence).items()}\n",
        "#         for word,count in repetition.items():\n",
        "#             for i in range(int((1-div)*count*2)):\n",
        "#                 if word in trend_sequence:trend_sequence.remove(word)\n",
        "#     new_sequences.append(trend_sequence)\n",
        "# test_sequences = new_sequences\n",
        "# new_sequences = []\n",
        "# for trend_sequence in validate_sequences:\n",
        "#     while 1 in trend_sequence:trend_sequence.remove(1)\n",
        "#     size_ = len(trend_sequence)\n",
        "#     if size_ > max_length_:\n",
        "#         div = max_length_/size_\n",
        "#         repetition = {item:count for item, count in collections.Counter(trend_sequence).items()}\n",
        "#         for word,count in repetition.items():\n",
        "#             for i in range(int((1-div)*count*2)):\n",
        "#                 if word in trend_sequence:trend_sequence.remove(word)\n",
        "#     new_sequences.append(trend_sequence)\n",
        "# validate_sequences = new_sequences\n",
        "\n",
        "\n",
        "\n",
        "# max_len1 = max_length(training_sequences)\n",
        "# max_len2 = max_length(test_sequences)\n",
        "# max_len3 = max_length(validate_sequences)\n",
        "# max_len = max(max_len1,max_len2,max_len3)\n",
        "max_len = 5000\n",
        "# print([max_len1,max_len2,max_len3])\n",
        "print(max_len)\n",
        "\n",
        "# Pad the sequence to have the same size\n",
        "Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "Xvalidate = pad_sequences(validate_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "# word_index = tokenizer.word_index\n",
        "# vocab_size = len(word_index)+1\n",
        "# print(vocab_size)\n",
        "\n",
        "\n",
        "emb_matrix = pretrained_embedding_matrix(word2vec, tokenizer, emb_mean, emb_std)\n",
        "\n",
        "# prinlt()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KsI1rRuvLGx"
      },
      "outputs": [],
      "source": [
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
        "                                             patience=20, verbose=2, \n",
        "                                             mode='max', restore_best_weights=True)\n",
        "# Define the input shape\n",
        "model = tcn_model(None, None, input_dim=TEXT_TOKENIZER_NUM_WORDS, \n",
        "                      max_length=max_len, emb_matrix=emb_matrix)\n",
        "# Train the model\n",
        "model.fit(Xtrain, y_train, batch_size=3, epochs=100, verbose=1, \n",
        "          callbacks=[callbacks], validation_data=(Xtest, y_test))\n",
        "loss, acc = model.evaluate(Xtest, y_test, verbose=0)\n",
        "print('Test Accuracy: {}'.format(acc*100))\n",
        "\n",
        "model.save('trend_prediction_model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cluster_trends(cluster_num,number_of_trends,classes_to_cluster_dict):\n",
        "    result = []\n",
        "    # print(classes_to_cluster_dict)\n",
        "    for trend,cluster in classes_to_cluster_dict.items():\n",
        "        if cluster == cluster_num:\n",
        "            result.append(trend)\n",
        "        if len(result) == number_of_trends:\n",
        "            break\n",
        "    return result\n",
        "\n",
        "\n",
        "def validate(x,ys,classes_to_cluster_dict):\n",
        "    y_preds = model.predict(np.array(x))\n",
        "    for index,y_pred in enumerate(y_preds):\n",
        "        y = ys[index]\n",
        "        print(\"-------------------------------\")\n",
        "        print(\"expected clusters :\")\n",
        "        # print(y)\n",
        "        for i in range(y.shape[0]):\n",
        "            if y[i] == 1.0:\n",
        "                print(f\"                   cluster {i} => {get_cluster_trends(i,6,classes_to_cluster_dict)}\")\n",
        "        print(\"predicted clusters :\")\n",
        "        \n",
        "        ind = np.argpartition(y_pred, -5)[-5:]\n",
        "        for i in ind:\n",
        "            print(f\"                   cluster {i} => {get_cluster_trends(i,6,classes_to_cluster_dict)}\")\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "validate(Xvalidate,y_validate,classes_to_cluster_dict)\n",
        "loss, acc = model.evaluate(Xvalidate, y_validate, verbose=0)\n",
        "print('validate Accuracy: {}'.format(acc*100))"
      ],
      "metadata": {
        "id": "XX4RhkmeMHtX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of trend_prediction_v1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}